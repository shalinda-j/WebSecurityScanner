"""
Malware Traffic Analysis Module for the Web Application Penetration Testing Toolkit.
This module provides capabilities for analyzing network traffic to detect, isolate,
and analyze potentially harmful software.
"""

import logging
import json
import os
import base64
import hashlib
import uuid
import tempfile
import time
import threading
from datetime import datetime
import requests
from urllib.parse import urlparse

logger = logging.getLogger(__name__)

class MalwareTrafficAnalyzer:
    """
    Module for analyzing network traffic to detect malware and suspicious behavior.
    """
    
    def __init__(self, config=None):
        """
        Initialize the Malware Traffic Analyzer with configuration
        
        Args:
            config (dict): Configuration parameters for the analyzer
        """
        self.config = config or {}
        self.target_url = self.config.get('url', '')
        self.analysis_mode = self.config.get('analysis_mode', 'passive')  # passive, active, or hybrid
        self.scan_depth = self.config.get('scan_depth', 'medium')  # light, medium, or deep
        self.session = requests.Session()
        self.artifacts = []
        self.analysis_id = str(uuid.uuid4())
        self.analysis_status = 'ready'
        self.current_stage = None
        self.temp_dir = tempfile.mkdtemp(prefix="malware_analysis_")
        
        # Set user agent to appear as legitimate browser
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36'
        })
        
        logger.info("Malware Traffic Analyzer initialized with mode: %s, depth: %s", 
                    self.analysis_mode, self.scan_depth)
    
    def scan(self, urls):
        """
        Scan network traffic for malware indicators and suspicious behavior
        
        Args:
            urls (list): List of URLs to analyze
            
        Returns:
            list: List of dictionaries containing vulnerability information
        """
        vulnerabilities = []
        
        for url in urls:
            try:
                parsed_url = urlparse(url)
                base_url = f"{parsed_url.scheme}://{parsed_url.netloc}"
                
                logger.info("Starting malware traffic analysis against %s", base_url)
                
                # Run analysis based on the selected mode
                if self.analysis_mode == 'active':
                    vuln_results = self._run_active_analysis(base_url)
                elif self.analysis_mode == 'hybrid':
                    vuln_results = self._run_hybrid_analysis(base_url)
                else:  # default to passive
                    vuln_results = self._run_passive_analysis(base_url)
                
                vulnerabilities.extend(vuln_results)
                
            except Exception as e:
                logger.error("Error during malware traffic analysis on %s: %s", url, str(e))
                vulnerabilities.append({
                    'type': 'Malware Analysis Error',
                    'severity': 'Info',
                    'description': f'Error occurred during analysis: {str(e)}',
                    'location': url,
                    'remediation': 'Check logs for details on the analysis error.'
                })
        
        return vulnerabilities
    
    def _run_passive_analysis(self, base_url):
        """
        Run passive analysis without triggering potentially malicious behavior
        
        Args:
            base_url (str): Base URL to analyze
            
        Returns:
            list: List of vulnerability findings
        """
        findings = []
        
        # Stage 1: Traffic Analysis
        self.current_stage = "traffic_analysis"
        logger.info("Stage 1: Performing passive traffic analysis")
        
        # Analyze HTTP headers
        header_findings = self._analyze_http_headers(base_url)
        findings.extend(header_findings)
        
        # Analyze response content
        content_findings = self._analyze_response_content(base_url)
        findings.extend(content_findings)
        
        # Stage 2: Static Resource Analysis
        self.current_stage = "static_analysis"
        logger.info("Stage 2: Performing static resource analysis")
        
        # Analyze JavaScript resources
        js_findings = self._analyze_javascript_resources(base_url)
        findings.extend(js_findings)
        
        # Analyze downloaded files
        file_findings = self._analyze_downloadable_resources(base_url)
        findings.extend(file_findings)
        
        # Stage 3: Communication Pattern Analysis
        self.current_stage = "communication_analysis"
        logger.info("Stage 3: Analyzing communication patterns")
        
        # Analyze external connections
        connection_findings = self._analyze_external_connections(base_url)
        findings.extend(connection_findings)
        
        # Update status to completed
        self.analysis_status = 'completed'
        
        return findings
    
    def _run_active_analysis(self, base_url):
        """
        Run active analysis including dynamic execution in isolated environment
        
        Args:
            base_url (str): Base URL to analyze
            
        Returns:
            list: List of vulnerability findings
        """
        findings = []
        
        # Include passive analysis results
        passive_findings = self._run_passive_analysis(base_url)
        findings.extend(passive_findings)
        
        # Stage 4: Dynamic Analysis (Sandbox)
        self.current_stage = "dynamic_analysis"
        logger.info("Stage 4: Performing dynamic analysis in sandbox")
        
        # Simulate execution of detected scripts
        dynamic_findings = self._simulate_script_execution(base_url)
        findings.extend(dynamic_findings)
        
        # Analyze behavior in sandbox
        behavior_findings = self._analyze_sandbox_behavior(base_url)
        findings.extend(behavior_findings)
        
        # Update status to completed
        self.analysis_status = 'completed'
        
        return findings
    
    def _run_hybrid_analysis(self, base_url):
        """
        Run hybrid analysis combining passive and selective active techniques
        
        Args:
            base_url (str): Base URL to analyze
            
        Returns:
            list: List of vulnerability findings
        """
        findings = []
        
        # Include passive analysis results
        passive_findings = self._run_passive_analysis(base_url)
        findings.extend(passive_findings)
        
        # Stage 4: Selective Dynamic Analysis
        self.current_stage = "selective_dynamic"
        logger.info("Stage 4: Performing selective dynamic analysis")
        
        # Identify high-risk components for selective dynamic analysis
        high_risk_components = self._identify_high_risk_components(base_url, passive_findings)
        
        # Perform targeted dynamic analysis on high-risk components
        if high_risk_components:
            targeted_findings = self._perform_targeted_dynamic_analysis(base_url, high_risk_components)
            findings.extend(targeted_findings)
        
        # Update status to completed
        self.analysis_status = 'completed'
        
        return findings
    
    def _analyze_http_headers(self, base_url):
        """
        Analyze HTTP headers for suspicious patterns
        
        Args:
            base_url (str): Base URL to analyze
            
        Returns:
            list: List of vulnerability findings related to HTTP headers
        """
        findings = []
        
        try:
            # Fetch headers from the target URL
            response = self.session.head(base_url, timeout=10)
            headers = response.headers
            
            # Check for suspicious security headers (or lack thereof)
            security_headers = {
                'Content-Security-Policy': False,
                'X-XSS-Protection': False,
                'X-Frame-Options': False,
                'X-Content-Type-Options': False,
                'Strict-Transport-Security': False
            }
            
            for header in security_headers:
                if header in headers:
                    security_headers[header] = True
            
            missing_headers = [h for h, present in security_headers.items() if not present]
            
            if missing_headers:
                findings.append({
                    'type': 'Missing Security Headers',
                    'severity': 'Medium',
                    'description': (
                        f'The site is missing important security headers: {", ".join(missing_headers)}. '
                        'This could make the site more vulnerable to certain types of attacks and malicious traffic.'
                    ),
                    'location': base_url,
                    'proof': json.dumps({
                        'missing_headers': missing_headers,
                        'existing_headers': dict(headers)
                    }),
                    'remediation': (
                        'Implement appropriate security headers to protect against common attack vectors. '
                        'Consider using a Content Security Policy to restrict resource loading and mitigate XSS attacks.'
                    )
                })
            
            # Check for suspicious headers that might indicate malware communication
            suspicious_headers = []
            unusual_headers = [h for h in headers if h.lower() not in [
                'server', 'date', 'content-type', 'content-length', 'connection', 'cache-control',
                'expires', 'pragma', 'set-cookie', 'x-powered-by', 'strict-transport-security',
                'content-security-policy', 'x-xss-protection', 'x-frame-options', 'x-content-type-options'
            ]]
            
            if unusual_headers:
                suspicious_headers.extend(unusual_headers)
            
            # Check for base64-encoded values in headers
            for header, value in headers.items():
                if len(value) > 20 and self._is_potential_base64(value):
                    suspicious_headers.append(f"{header} (potential Base64 encoding)")
            
            if suspicious_headers:
                findings.append({
                    'type': 'Suspicious HTTP Headers',
                    'severity': 'Medium',
                    'description': (
                        f'The site contains unusual HTTP headers that might be used for malware communication: {", ".join(suspicious_headers)}. '
                        'These headers could be used for data exfiltration or command-and-control.'
                    ),
                    'location': base_url,
                    'proof': json.dumps({
                        'suspicious_headers': suspicious_headers,
                        'headers': dict(headers)
                    }),
                    'remediation': (
                        'Investigate the purpose of these unusual headers. '
                        'If they are not required for legitimate functionality, they should be removed.'
                    )
                })
            
        except Exception as e:
            logger.error("Error analyzing HTTP headers: %s", str(e))
        
        return findings
    
    def _analyze_response_content(self, base_url):
        """
        Analyze response content for suspicious patterns
        
        Args:
            base_url (str): Base URL to analyze
            
        Returns:
            list: List of vulnerability findings related to response content
        """
        findings = []
        
        try:
            # Fetch content from the target URL
            response = self.session.get(base_url, timeout=10)
            content = response.text
            
            # Check for obfuscated JavaScript
            if 'eval(' in content or 'document.write(unescape(' in content or 'fromCharCode(' in content:
                findings.append({
                    'type': 'Obfuscated JavaScript',
                    'severity': 'High',
                    'description': (
                        'The site contains potentially obfuscated JavaScript code. '
                        'Obfuscation is commonly used to hide malicious code and evade detection.'
                    ),
                    'location': base_url,
                    'proof': json.dumps({
                        'obfuscation_indicators': [
                            {'type': 'eval', 'present': 'eval(' in content},
                            {'type': 'unescape', 'present': 'document.write(unescape(' in content},
                            {'type': 'fromCharCode', 'present': 'fromCharCode(' in content}
                        ]
                    }),
                    'remediation': (
                        'Review and analyze the obfuscated JavaScript to determine its purpose. '
                        'If it is legitimate, consider using more transparent methods. '
                        'If malicious, remove it immediately and investigate how it was introduced.'
                    )
                })
            
            # Check for hidden iframes
            if '<iframe' in content.lower() and ('style="display:none"' in content.lower() or 
                                                'style="visibility:hidden"' in content.lower() or 
                                                'style="opacity:0"' in content.lower()):
                findings.append({
                    'type': 'Hidden iFrame',
                    'severity': 'High',
                    'description': (
                        'The site contains hidden iframes, which are commonly used in drive-by download attacks '
                        'and malvertising campaigns to load malicious content without user awareness.'
                    ),
                    'location': base_url,
                    'proof': json.dumps({
                        'hidden_iframe_detected': True
                    }),
                    'remediation': (
                        'Investigate the purpose of hidden iframes. '
                        'Remove any unauthorized or suspicious iframes, especially those loading content from untrusted domains.'
                    )
                })
            
            # Check for suspicious redirects
            if 'window.location' in content or 'document.location' in content or 'location.href' in content:
                findings.append({
                    'type': 'Suspicious Redirects',
                    'severity': 'Medium',
                    'description': (
                        'The site contains JavaScript code that manipulates the browser location, which could be used for '
                        'malicious redirects to phishing sites or exploit kits.'
                    ),
                    'location': base_url,
                    'proof': json.dumps({
                        'redirect_code_detected': True
                    }),
                    'remediation': (
                        'Review all redirect code to ensure it points to legitimate destinations. '
                        'Consider implementing safer redirect methods with proper user notification.'
                    )
                })
            
            # Check for excessive Base64 encoded strings
            base64_strings = self._extract_base64_strings(content)
            if len(base64_strings) > 2:  # Multiple base64 strings might indicate encoding to hide malicious content
                findings.append({
                    'type': 'Excessive Base64 Encoding',
                    'severity': 'Medium',
                    'description': (
                        f'The site contains {len(base64_strings)} Base64-encoded strings, which might be used to obfuscate '
                        'malicious code or commands.'
                    ),
                    'location': base_url,
                    'proof': json.dumps({
                        'base64_count': len(base64_strings),
                        'sample': base64_strings[:3] if len(base64_strings) > 3 else base64_strings
                    }),
                    'remediation': (
                        'Decode and analyze Base64 strings to determine their purpose. '
                        'If not required for legitimate functionality, consider alternative approaches.'
                    )
                })
            
        except Exception as e:
            logger.error("Error analyzing response content: %s", str(e))
        
        return findings
    
    def _analyze_javascript_resources(self, base_url):
        """
        Analyze external and inline JavaScript resources
        
        Args:
            base_url (str): Base URL to analyze
            
        Returns:
            list: List of vulnerability findings related to JavaScript
        """
        findings = []
        
        try:
            # Fetch the main page to extract JavaScript resources
            response = self.session.get(base_url, timeout=10)
            content = response.text
            
            # Extract external script sources
            import re
            external_scripts = re.findall(r'<script[^>]*src=["\']([^"\']+)["\']', content)
            
            # Analyze each external script
            for script_src in external_scripts:
                # Resolve relative URLs
                if script_src.startswith('//'):
                    script_url = f"{urlparse(base_url).scheme}:{script_src}"
                elif not script_src.startswith(('http://', 'https://')):
                    script_url = f"{base_url.rstrip('/')}/{script_src.lstrip('/')}"
                else:
                    script_url = script_src
                
                try:
                    script_response = self.session.get(script_url, timeout=10)
                    script_content = script_response.text
                    
                    # Check for suspicious patterns in the script
                    suspicious_patterns = self._check_script_for_suspicious_patterns(script_content)
                    
                    if suspicious_patterns:
                        findings.append({
                            'type': 'Suspicious JavaScript',
                            'severity': 'High',
                            'description': (
                                f'External JavaScript resource at {script_url} contains suspicious patterns '
                                'that could indicate malicious activity.'
                            ),
                            'location': script_url,
                            'proof': json.dumps({
                                'suspicious_patterns': suspicious_patterns,
                                'script_url': script_url
                            }),
                            'remediation': (
                                'Review the flagged JavaScript resource for malicious code. '
                                'If confirmed malicious, remove it and investigate how it was introduced. '
                                'Consider implementing Subresource Integrity (SRI) checks for external scripts.'
                            )
                        })
                    
                except Exception as script_error:
                    logger.error("Error analyzing script %s: %s", script_url, str(script_error))
            
            # Extract and analyze inline scripts
            inline_scripts = re.findall(r'<script[^>]*>(.*?)</script>', content, re.DOTALL)
            
            for i, script in enumerate(inline_scripts):
                if len(script.strip()) > 0:  # Skip empty scripts
                    suspicious_patterns = self._check_script_for_suspicious_patterns(script)
                    
                    if suspicious_patterns:
                        findings.append({
                            'type': 'Suspicious Inline JavaScript',
                            'severity': 'High',
                            'description': (
                                f'Inline script #{i+1} on page {base_url} contains suspicious patterns '
                                'that could indicate malicious activity.'
                            ),
                            'location': f"{base_url}#inline-script-{i+1}",
                            'proof': json.dumps({
                                'suspicious_patterns': suspicious_patterns,
                                'script_index': i+1
                            }),
                            'remediation': (
                                'Review the flagged inline script for malicious code. '
                                'If confirmed malicious, remove it and investigate how it was introduced. '
                                'Consider implementing a Content Security Policy (CSP) to restrict inline scripts.'
                            )
                        })
            
        except Exception as e:
            logger.error("Error analyzing JavaScript resources: %s", str(e))
        
        return findings
    
    def _analyze_downloadable_resources(self, base_url):
        """
        Analyze downloadable files for potential malware indicators
        
        Args:
            base_url (str): Base URL to analyze
            
        Returns:
            list: List of vulnerability findings related to downloadable files
        """
        findings = []
        
        try:
            # Fetch the main page to extract downloadable resources
            response = self.session.get(base_url, timeout=10)
            content = response.text
            
            # Extract download links
            import re
            download_links = re.findall(r'<a[^>]*href=["\']([^"\']+\.(exe|zip|rar|msi|dmg|pkg|deb|rpm))["\']', content, re.IGNORECASE)
            download_links = [link[0] for link in download_links]  # Extract just the URL
            
            # Check for suspicious download links
            for link in download_links:
                # Resolve relative URLs
                if link.startswith('//'):
                    download_url = f"{urlparse(base_url).scheme}:{link}"
                elif not link.startswith(('http://', 'https://')):
                    download_url = f"{base_url.rstrip('/')}/{link.lstrip('/')}"
                else:
                    download_url = link
                
                # We won't actually download the files, just analyze the URLs
                file_extension = download_url.split('.')[-1].lower()
                file_name = download_url.split('/')[-1]
                
                # Check if the download URL is suspicious
                suspicious_indicators = []
                
                # Check for unusual domains
                parsed_url = urlparse(download_url)
                domain = parsed_url.netloc
                
                if domain and domain != urlparse(base_url).netloc:
                    suspicious_indicators.append(f"Download hosted on external domain {domain}")
                
                # Check for unusual file naming patterns
                if re.match(r'^[a-zA-Z0-9]{10,}$', file_name.split('.')[0]):
                    suspicious_indicators.append("Random or unusually long file name")
                
                # Check for executable content
                if file_extension in ['exe', 'msi']:
                    suspicious_indicators.append("Executable file type")
                
                if suspicious_indicators:
                    findings.append({
                        'type': 'Suspicious Downloadable File',
                        'severity': 'High',
                        'description': (
                            f'The site links to potentially suspicious downloadable file: {file_name}. '
                            f'Suspicious indicators: {", ".join(suspicious_indicators)}.'
                        ),
                        'location': download_url,
                        'proof': json.dumps({
                            'file_name': file_name,
                            'file_extension': file_extension,
                            'download_url': download_url,
                            'suspicious_indicators': suspicious_indicators
                        }),
                        'remediation': (
                            'Verify the legitimacy of the downloadable file. '
                            'If it is not an authorized file, remove the link immediately. '
                            'Consider implementing file integrity checks for downloadable content.'
                        )
                    })
            
        except Exception as e:
            logger.error("Error analyzing downloadable resources: %s", str(e))
        
        return findings
    
    def _analyze_external_connections(self, base_url):
        """
        Analyze external connections and resources referenced by the site
        
        Args:
            base_url (str): Base URL to analyze
            
        Returns:
            list: List of vulnerability findings related to external connections
        """
        findings = []
        
        try:
            # Fetch the main page to extract external connections
            response = self.session.get(base_url, timeout=10)
            content = response.text
            
            # Extract all external domains referenced in scripts, links, iframes, etc.
            import re
            external_urls = []
            
            # Extract from script src
            external_urls.extend(re.findall(r'<script[^>]*src=["\'](?:https?:)?//([^/"\':]+)[^"\']*["\']', content))
            
            # Extract from link href
            external_urls.extend(re.findall(r'<link[^>]*href=["\'](?:https?:)?//([^/"\':]+)[^"\']*["\']', content))
            
            # Extract from img src
            external_urls.extend(re.findall(r'<img[^>]*src=["\'](?:https?:)?//([^/"\':]+)[^"\']*["\']', content))
            
            # Extract from iframe src
            external_urls.extend(re.findall(r'<iframe[^>]*src=["\'](?:https?:)?//([^/"\':]+)[^"\']*["\']', content))
            
            # Extract absolute URLs from JavaScript
            external_urls.extend(re.findall(r'["\'](?:https?:)?//([^/"\':]+)[^"\']*["\']', content))
            
            # Filter out the base domain and remove duplicates
            base_domain = urlparse(base_url).netloc
            if base_domain.startswith('www.'):
                base_domain = base_domain[4:]
                
            external_domains = set()
            for domain in external_urls:
                if domain.startswith('www.'):
                    domain = domain[4:]
                if domain != base_domain and domain not in external_domains:
                    external_domains.add(domain)
            
            # Check for suspicious domains
            suspicious_domains = []
            for domain in external_domains:
                # Check for randomly generated domain patterns
                if re.match(r'^[a-z0-9]{10,}\.[a-z]{2,}$', domain):
                    suspicious_domains.append({
                        'domain': domain,
                        'reason': 'Potentially algorithmically generated domain'
                    })
                
                # Check for unusual TLDs often associated with malware
                if domain.split('.')[-1] in ['top', 'xyz', 'tk', 'ml', 'ga', 'cf', 'gq']:
                    suspicious_domains.append({
                        'domain': domain,
                        'reason': f"Unusual TLD (.{domain.split('.')[-1]}) often associated with malicious activity"
                    })
                
                # Check for domain typosquatting common services
                common_services = ['google', 'facebook', 'microsoft', 'apple', 'amazon', 'paypal']
                for service in common_services:
                    if service in domain and not domain.endswith(f"{service}.com") and not domain.endswith(f"{service}.org"):
                        suspicious_domains.append({
                            'domain': domain,
                            'reason': f"Potential typosquatting of {service}"
                        })
            
            if suspicious_domains:
                findings.append({
                    'type': 'Suspicious External Connections',
                    'severity': 'High',
                    'description': (
                        f'The site connects to potentially suspicious external domains. '
                        f'This could indicate malware communication channels or malicious resource loading.'
                    ),
                    'location': base_url,
                    'proof': json.dumps({
                        'suspicious_domains': suspicious_domains,
                        'total_external_domains': len(external_domains)
                    }),
                    'remediation': (
                        'Investigate all suspicious external connections. '
                        'Remove any unauthorized or malicious domains. '
                        'Implement a Content Security Policy to restrict resource loading to trusted domains.'
                    )
                })
            
            # If there are many external domains, note it as a potential issue
            if len(external_domains) > 15:  # Arbitrary threshold, adjust as needed
                findings.append({
                    'type': 'Excessive External Connections',
                    'severity': 'Medium',
                    'description': (
                        f'The site connects to an unusually high number of external domains ({len(external_domains)}). '
                        'While not necessarily malicious, this increases the attack surface and risk of supply chain attacks.'
                    ),
                    'location': base_url,
                    'proof': json.dumps({
                        'external_domain_count': len(external_domains),
                        'sample_domains': list(external_domains)[:10]
                    }),
                    'remediation': (
                        'Review all external connections to ensure they are necessary. '
                        'Consolidate resources where possible and implement subresource integrity checks. '
                        'Consider self-hosting critical resources to reduce dependency on external domains.'
                    )
                })
            
        except Exception as e:
            logger.error("Error analyzing external connections: %s", str(e))
        
        return findings
    
    def _simulate_script_execution(self, base_url):
        """
        Simulate execution of scripts in a sandbox environment
        
        Args:
            base_url (str): Base URL to analyze
            
        Returns:
            list: List of vulnerability findings related to script behavior
        """
        # This would typically use a headless browser or JS engine in a sandbox
        # For demonstration, we'll just return a placeholder
        findings = []
        
        findings.append({
            'type': 'Dynamic Analysis Simulation',
            'severity': 'Info',
            'description': (
                'Dynamic analysis would execute scripts in an isolated environment to observe behavior. '
                'This requires a JavaScript engine or headless browser with proper sandboxing.'
            ),
            'location': base_url,
            'proof': json.dumps({
                'simulation_required': True,
                'note': 'Actual implementation would require a sandboxed JS engine'
            }),
            'remediation': (
                'For complete malware detection, implement a sandboxed execution environment '
                'with proper monitoring of DOM manipulation, network requests, and system API calls.'
            )
        })
        
        return findings
    
    def _analyze_sandbox_behavior(self, base_url):
        """
        Analyze behavior of scripts in sandbox environment
        
        Args:
            base_url (str): Base URL to analyze
            
        Returns:
            list: List of vulnerability findings related to sandbox behavior
        """
        # This would analyze the results from the sandbox execution
        # For demonstration, we'll just return a placeholder
        findings = []
        
        findings.append({
            'type': 'Sandbox Behavior Analysis',
            'severity': 'Info',
            'description': (
                'Sandbox behavior analysis would monitor API calls, network connections, and DOM modifications '
                'during script execution to identify malicious patterns.'
            ),
            'location': base_url,
            'proof': json.dumps({
                'implementation_required': True,
                'note': 'Actual implementation would require sandbox environment and behavior monitoring'
            }),
            'remediation': (
                'Implement a sandbox environment with full monitoring capabilities to '
                'detect malicious behaviors including process injection, persistence techniques, and evasion methods.'
            )
        })
        
        return findings
    
    def _identify_high_risk_components(self, base_url, passive_findings):
        """
        Identify high-risk components for selective dynamic analysis
        
        Args:
            base_url (str): Base URL to analyze
            passive_findings (list): Results from passive analysis
            
        Returns:
            list: High-risk components that warrant further dynamic analysis
        """
        high_risk_components = []
        
        # Extract suspicious components from passive findings
        for finding in passive_findings:
            if finding['severity'] in ['High', 'Critical']:
                try:
                    proof = json.loads(finding['proof'])
                    
                    if finding['type'] == 'Obfuscated JavaScript':
                        high_risk_components.append({
                            'type': 'script',
                            'location': finding['location'],
                            'risk_factor': 'obfuscation'
                        })
                    elif finding['type'] == 'Hidden iFrame':
                        high_risk_components.append({
                            'type': 'iframe',
                            'location': finding['location'],
                            'risk_factor': 'hidden_element'
                        })
                    elif finding['type'] == 'Suspicious External Connections':
                        for domain in proof.get('suspicious_domains', []):
                            high_risk_components.append({
                                'type': 'domain',
                                'location': domain['domain'],
                                'risk_factor': domain['reason']
                            })
                    elif 'Suspicious JavaScript' in finding['type']:
                        high_risk_components.append({
                            'type': 'script',
                            'location': finding['location'],
                            'risk_factor': 'suspicious_patterns'
                        })
                except Exception as e:
                    logger.error("Error processing finding for high-risk components: %s", str(e))
        
        return high_risk_components
    
    def _perform_targeted_dynamic_analysis(self, base_url, high_risk_components):
        """
        Perform targeted dynamic analysis on high-risk components
        
        Args:
            base_url (str): Base URL to analyze
            high_risk_components (list): Components identified as high risk
            
        Returns:
            list: List of vulnerability findings from targeted dynamic analysis
        """
        # This would analyze specific high-risk components in a sandbox
        # For demonstration, we'll just return a placeholder
        findings = []
        
        for component in high_risk_components:
            findings.append({
                'type': 'Targeted Dynamic Analysis',
                'severity': 'High',
                'description': (
                    f'Targeted analysis of high-risk {component["type"]} at {component["location"]} would be performed. '
                    f'This component was flagged due to: {component["risk_factor"]}.'
                ),
                'location': component['location'],
                'proof': json.dumps({
                    'component_type': component['type'],
                    'risk_factor': component['risk_factor'],
                    'note': 'Actual implementation would require targeted sandbox analysis'
                }),
                'remediation': (
                    'Review and investigate the identified high-risk component. '
                    'If confirmed malicious, remove or replace it with a safe alternative.'
                )
            })
        
        return findings
    
    def _check_script_for_suspicious_patterns(self, script_content):
        """
        Check JavaScript content for suspicious patterns
        
        Args:
            script_content (str): JavaScript content to analyze
            
        Returns:
            list: List of suspicious patterns found
        """
        suspicious_patterns = []
        
        # Check for eval or similar dynamic code execution
        if 'eval(' in script_content:
            suspicious_patterns.append({
                'pattern': 'eval()',
                'description': 'Dynamic code execution function that can be used to run obfuscated code'
            })
        
        # Check for document.write with encoded content
        if 'document.write(unescape(' in script_content or 'document.write(atob(' in script_content:
            suspicious_patterns.append({
                'pattern': 'document.write() with encoded content',
                'description': 'Writing decoded content to the page, often used to hide malicious code'
            })
        
        # Check for fromCharCode obfuscation
        if 'fromCharCode(' in script_content:
            suspicious_patterns.append({
                'pattern': 'String.fromCharCode()',
                'description': 'Character code conversion often used to obfuscate strings in malicious code'
            })
        
        # Check for iframe creation
        if 'createElement("iframe")' in script_content or "createElement('iframe')" in script_content:
            suspicious_patterns.append({
                'pattern': 'Dynamic iframe creation',
                'description': 'Creating iframes via JavaScript, potentially to load malicious content'
            })
        
        # Check for browser fingerprinting
        fingerprinting_indicators = [
            'navigator.userAgent', 'navigator.language', 'navigator.platform',
            'screen.width', 'screen.height', 'navigator.plugins'
        ]
        found_indicators = [i for i in fingerprinting_indicators if i in script_content]
        if len(found_indicators) >= 3:
            suspicious_patterns.append({
                'pattern': 'Browser fingerprinting',
                'description': f'Collecting browser information ({len(found_indicators)} indicators), potentially for tracking or targeting'
            })
        
        # Check for localStorage or sessionStorage access
        if 'localStorage' in script_content or 'sessionStorage' in script_content:
            suspicious_patterns.append({
                'pattern': 'Web Storage API usage',
                'description': 'Accessing browser storage, potentially for persistence or data exfiltration'
            })
        
        # Check for WebSocket connections
        if 'new WebSocket(' in script_content:
            suspicious_patterns.append({
                'pattern': 'WebSocket connection',
                'description': 'Establishing WebSocket connection, potentially for command and control'
            })
        
        # Check for excessive encoding/decoding
        encoding_functions = ['escape', 'unescape', 'encodeURI', 'decodeURI', 'btoa', 'atob']
        encoding_count = sum(script_content.count(func) for func in encoding_functions)
        if encoding_count > 5:
            suspicious_patterns.append({
                'pattern': 'Excessive encoding/decoding',
                'description': f'Found {encoding_count} instances of encoding/decoding functions, potentially to obfuscate malicious code'
            })
        
        return suspicious_patterns
    
    def _extract_base64_strings(self, content):
        """
        Extract potential Base64 encoded strings from content
        
        Args:
            content (str): Content to analyze
            
        Returns:
            list: List of potential Base64 strings
        """
        import re
        # Look for typical base64 pattern (excluding data URI prefixes)
        base64_pattern = r'(?:[A-Za-z0-9+/]{4})*(?:[A-Za-z0-9+/]{2}==|[A-Za-z0-9+/]{3}=)?'
        # Only find strings of reasonable length to avoid false positives
        potential_base64 = re.findall(f'{base64_pattern}', content)
        
        # Filter for strings that are likely Base64 (reasonable length and proper character set)
        valid_base64 = []
        for string in potential_base64:
            if len(string) >= 20 and self._is_potential_base64(string):
                valid_base64.append(string)
        
        return valid_base64
    
    def _is_potential_base64(self, string):
        """
        Check if a string is potentially Base64 encoded
        
        Args:
            string (str): String to check
            
        Returns:
            bool: True if the string could be Base64, False otherwise
        """
        import re
        # Check if string matches Base64 pattern
        if not re.match(r'^[A-Za-z0-9+/]+={0,2}$', string):
            return False
        
        # Check if length is valid (multiple of 4, or padded correctly)
        if len(string) % 4 != 0 and not string.endswith(('=', '==')):
            return False
        
        # Check character distribution (should have a reasonable mix)
        char_counts = {}
        for char in string:
            if char not in char_counts:
                char_counts[char] = 0
            char_counts[char] += 1
        
        unique_chars = len(char_counts)
        if unique_chars < 10:  # Arbitrary threshold, adjust as needed
            return False
        
        return True
    
    def get_analysis_modes(self):
        """
        Get list of available analysis modes
        
        Returns:
            list: Descriptions of available analysis modes
        """
        modes = [
            {
                'id': 'passive',
                'name': 'Passive Analysis',
                'description': 'Analyze without executing code or triggering potentially malicious behavior',
                'safety_level': 'High'
            },
            {
                'id': 'active',
                'name': 'Active Analysis',
                'description': 'Includes dynamic execution in isolated environment for deeper behavior analysis',
                'safety_level': 'Medium'
            },
            {
                'id': 'hybrid',
                'name': 'Hybrid Analysis',
                'description': 'Combines passive analysis with selective active analysis of high-risk components',
                'safety_level': 'Medium-High'
            }
        ]
        
        return modes
    
    def get_scan_depths(self):
        """
        Get list of available scan depths
        
        Returns:
            list: Descriptions of available scan depths
        """
        depths = [
            {
                'id': 'light',
                'name': 'Light Scan',
                'description': 'Quick analysis of main page and primary resources',
                'duration_estimate': 'Fast (1-2 minutes)'
            },
            {
                'id': 'medium',
                'name': 'Medium Scan',
                'description': 'Thorough analysis of main page and all directly linked resources',
                'duration_estimate': 'Moderate (3-5 minutes)'
            },
            {
                'id': 'deep',
                'name': 'Deep Scan',
                'description': 'Comprehensive analysis including crawling linked pages and recursive resource analysis',
                'duration_estimate': 'Slow (5-15 minutes)'
            }
        ]
        
        return depths
    
    def get_status(self):
        """
        Get current status of the malware traffic analysis
        
        Returns:
            dict: Current status information
        """
        return {
            'analysis_id': self.analysis_id,
            'status': self.analysis_status,
            'current_stage': self.current_stage,
            'artifacts_created': len(self.artifacts),
            'start_time': self.analysis_start_time if hasattr(self, 'analysis_start_time') else None,
            'elapsed_time': time.time() - self.analysis_start_time if hasattr(self, 'analysis_start_time') else 0
        }
    
    def start_analysis(self, analysis_mode=None, scan_depth=None):
        """
        Start a standalone malware traffic analysis (not part of scanner)
        
        Args:
            analysis_mode (str): Analysis mode to use
            scan_depth (str): Scan depth to use
            
        Returns:
            dict: Initial analysis information
        """
        if analysis_mode:
            self.analysis_mode = analysis_mode
        if scan_depth:
            self.scan_depth = scan_depth
        
        self.analysis_status = 'running'
        self.analysis_start_time = time.time()
        
        return {
            'analysis_id': self.analysis_id,
            'analysis_mode': self.analysis_mode,
            'scan_depth': self.scan_depth,
            'status': 'started',
            'start_time': self.analysis_start_time
        }
    
    def stop_analysis(self):
        """
        Stop an ongoing analysis
        
        Returns:
            dict: Analysis results summary
        """
        self.analysis_status = 'stopped'
        
        return {
            'analysis_id': self.analysis_id,
            'status': 'stopped',
            'elapsed_time': time.time() - self.analysis_start_time if hasattr(self, 'analysis_start_time') else 0,
            'artifacts_created': len(self.artifacts)
        }
    
    def generate_report(self):
        """
        Generate a comprehensive report of the malware traffic analysis
        
        Returns:
            dict: Detailed analysis report
        """
        # This would be expanded in a real implementation
        return {
            'analysis_id': self.analysis_id,
            'analysis_mode': self.analysis_mode,
            'scan_depth': self.scan_depth,
            'status': self.analysis_status,
            'findings_count': len(self.artifacts),
            'recommendations': [
                'Implement a Content Security Policy to restrict resource loading',
                'Use Subresource Integrity checks for external scripts',
                'Monitor network traffic for suspicious connections',
                'Keep software and dependencies updated to patch known vulnerabilities',
                'Deploy behavior-based malware detection on endpoints'
            ]
        }
    
    def cleanup(self):
        """
        Clean up temporary files and resources
        """
        try:
            import shutil
            if os.path.exists(self.temp_dir):
                shutil.rmtree(self.temp_dir)
            logger.info(f"Cleaned up temporary directory {self.temp_dir}")
        except Exception as e:
            logger.error(f"Error cleaning up: {str(e)}")